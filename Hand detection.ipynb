{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start streaming...\n"
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "W = 848\n",
    "H = 480\n",
    "\n",
    "# Configure depth and color streams\n",
    "sn='031222070617'\n",
    "\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_device(sn)\n",
    "#config.enable_stream(rs.stream.depth, W, H, rs.format.z16, 30)\n",
    "#config.enable_stream(rs.stream.color, W, H, rs.format.bgr8, 30)\n",
    "\n",
    "print(\"[INFO] start streaming...\")\n",
    "pipeline.start(config)\n",
    "\n",
    "aligned_stream = rs.align(rs.stream.color) # alignment between color and depth\n",
    "point_cloud = rs.pointcloud()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading yolo...\n",
      "starting webcam...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "\n",
    "from yolo import YOLO\n",
    "\n",
    "\n",
    "network=\"normal\"\n",
    "hands=2\n",
    "device = 0\n",
    "if network == \"normal\":\n",
    "    print(\"loading yolo...\")\n",
    "    yolo = YOLO(\"models/cross-hands.cfg\", \"models/cross-hands.weights\", [\"hand\"])\n",
    "elif network == \"prn\":\n",
    "    print(\"loading yolo-tiny-prn...\")\n",
    "    yolo = YOLO(\"models/cross-hands-tiny-prn.cfg\", \"models/cross-hands-tiny-prn.weights\", [\"hand\"])\n",
    "elif network == \"v4-tiny\":\n",
    "    print(\"loading yolov4-tiny-prn...\")\n",
    "    yolo = YOLO(\"models/cross-hands-yolov4-tiny.cfg\", \"models/cross-hands-yolov4-tiny.weights\", [\"hand\"])\n",
    "else:\n",
    "    print(\"loading yolo-tiny...\")\n",
    "    yolo = YOLO(\"models/cross-hands-tiny.cfg\", \"models/cross-hands-tiny.weights\", [\"hand\"])\n",
    "\n",
    "yolo.size = int(416)\n",
    "yolo.confidence = float(0.2)\n",
    "\n",
    "print(\"starting webcam...\")\n",
    "cv2.namedWindow(\"preview\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "if vc.isOpened():  # try to get the first frame\n",
    "    rval, frame = vc.read()\n",
    "else:\n",
    "    rval = False\n",
    "\n",
    "while rval:\n",
    "    width, height, inference_time, results = yolo.inference(frame)\n",
    "\n",
    "    # display fps\n",
    "    cv2.putText(frame, f'{round(1/inference_time,2)} FPS', (15,15), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,255,255), 2)\n",
    "\n",
    "    # sort by confidence\n",
    "    results.sort(key=lambda x: x[2])\n",
    "\n",
    "    # how many hands should be shown\n",
    "    hand_count = len(results)\n",
    "    if hands != -1:\n",
    "        hand_count = int(hands)\n",
    "\n",
    "    # display hands\n",
    "    for detection in results[:hand_count]:\n",
    "        id, name, confidence, x, y, w, h = detection\n",
    "        cx = x + (w / 2)\n",
    "        cy = y + (h / 2)\n",
    "\n",
    "        # draw a bounding box rectangle and label on the image\n",
    "        color = (0, 255, 255)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        xmin = x\n",
    "        ymin = y\n",
    "        xmax = x+w\n",
    "        ymax = y+h\n",
    "        \n",
    "        text = \"%s (%s)\" % (name, round(confidence, 2))\n",
    "        cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 2)\n",
    "\n",
    "    cv2.imshow(\"preview\", frame)\n",
    "\n",
    "    rval, frame = vc.read()\n",
    "\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27:  # exit on ESC\n",
    "        break\n",
    "\n",
    "cv2.destroyWindow(\"preview\")\n",
    "vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from yolo import YOLO\n",
    "yolo = YOLO(\"models/cross-hands-tiny.cfg\", \"models/cross-hands-tiny.weights\", [\"hand\"])\n",
    "yolo.size = int(256)\n",
    "yolo.confidence = float(0.2)\n",
    "def getHandDetection(frame,depth_frame,hands):\n",
    "    \n",
    "    width, height, inference_time, results = yolo.inference(frame)\n",
    "\n",
    "    # display fps\n",
    "    cv2.putText(frame, f'{round(1/inference_time,2)} FPS', (15,15), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,255,255), 2)\n",
    "\n",
    "    # sort by confidence\n",
    "    results.sort(key=lambda x: x[2])\n",
    "\n",
    "    # how many hands should be shown\n",
    "    hand_count = len(results)\n",
    "    if hands != -1:\n",
    "        hand_count = int(hands)\n",
    "\n",
    "    # display hands\n",
    "    for detection in results[:hand_count]:\n",
    "        id, name, confidence, x, y, w, h = detection\n",
    "        cx = x + (w / 2)\n",
    "        cy = y + (h / 2)\n",
    "\n",
    "        # draw a bounding box rectangle and label on the image\n",
    "        color = (0, 255, 255)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        xmin_depth = x\n",
    "        ymin_depth = y\n",
    "        xmax_depth = x+w\n",
    "        ymax_depth = y+h\n",
    "        depth = np.asanyarray(depth_frame.get_data())\n",
    "        # Crop depth data:\n",
    "        depth = depth[xmin_depth:xmax_depth,ymin_depth:ymax_depth].astype(float)\n",
    "        showBoxonDepth(depth_frame, xmin_depth,xmax_depth,ymin_depth,ymax_depth)\n",
    "\n",
    "        # Get data scale from the device and convert to meters\n",
    "        depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "        depth = depth * depth_scale\n",
    "        dist,_,_,_ = cv2.mean(depth)\n",
    "        #print(\"Detected {0:.3} meters away.\".format(dist))\n",
    "        distanceString = \"Detected {0:.3} meters away.\".format(dist)\n",
    "        text = \"%s (%s)\" % (name, round(confidence, 2))\n",
    "        cv2.putText(frame, text+distanceString, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 2)\n",
    "    \n",
    "    cv2.imshow(\"preview\", frame)\n",
    "    \n",
    "    return frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    " \n",
    "# fontScale\n",
    "fontScale = 1\n",
    "   \n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "  \n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "def getFingers(frame):\n",
    "    with mp_hands.Hands(min_detection_confidence=0.6, min_tracking_confidence=0.5) as hands:\n",
    "        \n",
    "\n",
    "        # BGR 2 RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Flip on horizontal\n",
    "        #image = cv2.flip(image, 1)\n",
    "\n",
    "        # Set flag\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Detections\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Set flag to true\n",
    "        image.flags.writeable = True\n",
    "\n",
    "        # RGB 2 BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detections\n",
    "        #print(results)\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "\n",
    "        # Rendering results\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                xIndexFinger =hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width\n",
    "                yIndexFinger = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y* image_height\n",
    "                xMiddleFinger =hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].x * image_width\n",
    "                yMiddleFinger = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y* image_height\n",
    "                cv2.putText(image, 'o', (int(xIndexFinger)-10,int(yIndexFinger)-10), font, fontScale, (255, 0, 0), 4, cv2.LINE_AA)\n",
    "                #cv2.putText(image, 'o', (int(xMiddleFinger)-10,int(yMiddleFinger)-10), font, fontScale, (255, 255, 0), 4, cv2.LINE_AA)\n",
    "                return image, xIndexFinger, yIndexFinger\n",
    "        return image, False, False\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start streaming...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Frame didn't arrive within 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-072d4fcd5111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mpoint_cloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpointcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Frame didn't arrive within 5000"
     ]
    }
   ],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "W = 848\n",
    "H = 480\n",
    "\n",
    "from yolo import YOLO\n",
    "#yolo = YOLO(\"models/cross-hands.cfg\", \"models/cross-hands.weights\", [\"hand\"])\n",
    "yolo = YOLO(\"models/cross-hands-tiny.cfg\", \"models/cross-hands-tiny.weights\", [\"hand\"])\n",
    "yolo.size = int(416)\n",
    "yolo.confidence = float(0.2)\n",
    "\n",
    "\n",
    "sn='031222070617'\n",
    "\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_device(sn)\n",
    "\n",
    "#config.enable_stream(rs.stream.depth, W, H, rs.format.z16, 30)\n",
    "#config.enable_stream(rs.stream.color, W, H, rs.format.bgr8, 30)\n",
    "\n",
    "dec_filter = rs.decimation_filter()  # Decimation-reduce df density\n",
    "spat_filter = rs.spatial_filter()    # Spatial-spatial smoothing\n",
    "temp_filter = rs.temporal_filter()   # Temporal-reduces temporal noise\n",
    "\n",
    "print(\"[INFO] start streaming...\")\n",
    "profile = pipeline.start(config)\n",
    "#device = profile.get_device()\n",
    "#depth_sensor = device.first_depth_sensor()\n",
    "#device.hardware_reset()\n",
    "\n",
    "depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "align = rs.align(rs.stream.color)\n",
    "point_cloud = rs.pointcloud()\n",
    "for x in range(10):\n",
    "    pipeline.wait_for_frames()\n",
    "\n",
    "\n",
    "frames = pipeline.wait_for_frames()\n",
    "#frames = aligned_stream.process(frames)\n",
    "depth_frame = frames.get_depth_frame()\n",
    "color_frame = frames.get_color_frame()\n",
    "#points = point_cloud.calculate(depth_frame)\n",
    "#color_frame = frameset_before.get_color_frame()\n",
    "frames = dec_filter.process(frames).as_frameset()\n",
    "frames = spat_filter.process(frames).as_frameset()\n",
    "frames = temp_filter.process(frames).as_frameset()\n",
    "\n",
    "\n",
    "while True:\n",
    "    frames = pipeline.wait_for_frames()\n",
    "   \n",
    "    \n",
    "    \n",
    "    aligned_frames = align.process(frames)       # Get aligned frames\n",
    "    depth_frame = aligned_frames.get_depth_frame()\n",
    "    color_frame = frames.get_color_frame()\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    #verts = np.asanyarray(points.get_vertices()).view(np.float32).reshape(-1, W, 3)  # xyz\n",
    "    # Convert images to numpy arrays\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    \n",
    "    frame = color_image\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hands = 1\n",
    "    \n",
    "    colorizer = rs.colorizer()\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "    depth = np.asanyarray(depth_frame.get_data())\n",
    "    # Get data scale from the device and convert to meters\n",
    "    \n",
    "    \n",
    "    width, height, inference_time, results = yolo.inference(frame)\n",
    "    frame,indexX,indexY = getFingers(frame)\n",
    "\n",
    "\n",
    "    # display fps\n",
    "    cv2.putText(frame, f'{round(1/inference_time,2)} FPS', (15,15), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,255,255), 2)\n",
    "\n",
    "    # sort by confidence\n",
    "    results.sort(key=lambda x: x[2])\n",
    "\n",
    "    # how many hands should be shown\n",
    "    hand_count = len(results)\n",
    "    if hands != -1:\n",
    "        hand_count = int(hands)\n",
    "   \n",
    "\n",
    "    # display hands\n",
    "    for detection in results[:hand_count]:\n",
    "        id, name, confidence, x, y, w, h = detection\n",
    "        cx = x + (w / 2)\n",
    "        cy = y + (h / 2)\n",
    "\n",
    "        # draw a bounding box rectangle and label on the image\n",
    "        color = (0, 255, 255)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        xmin_depth = x+round(w/4)\n",
    "        ymin_depth = y+round(h/4)\n",
    "        xmax_depth = x+round(w*.75)\n",
    "        ymax_depth = y+round(h*.75)      \n",
    "        \n",
    "        \n",
    "        \n",
    "        #depth = np.asanyarray(depth_frame.get_data())\n",
    "        # Crop depth data:\n",
    "        depth_hand = depth[ymin_depth:ymax_depth,xmin_depth:xmax_depth].astype(float)\n",
    "        depth_hand_scaled = depth_hand * depth_scale\n",
    "        dist,_,_,_ = cv2.mean(depth_hand_scaled)\n",
    "        #print(\"Detected {0:.3} meters away.\".format(dist))\n",
    "        distanceString = \"Detected {0:.3} meters away.\".format(dist)\n",
    "        \n",
    "        \n",
    "        cv2.rectangle(colorized_depth, (xmin_depth, ymin_depth), \n",
    "                     (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.putText(colorized_depth, distanceString, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 2)\n",
    "        cv2.imshow(\"Depth Frame\",colorized_depth)\n",
    "        \n",
    "        text = \"%s (%s)\" % (name, round(confidence, 2))\n",
    "        cv2.putText(frame, text+distanceString, (15, 65), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 2)\n",
    "        images = np.hstack((frame, colorized_depth))\n",
    "        plt.imshow(images)\n",
    "    \n",
    "    if indexX:\n",
    "        indexDist=(depth[int(indexY),int(indexX)].astype(float))*depth_scale\n",
    "        iDistanceSTR = \"Detected {0:.3} meters away.\".format(indexDist)\n",
    "        cv2.putText(frame,\"Index Finger \"+ iDistanceSTR,(15,35),cv2.FONT_HERSHEY_SIMPLEX,0.5, (255, 0, 0), 2 )\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    cv2.imshow(\"preview\", frame)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# expand image dimensions to have shape: [1, None, None, 3]\n",
    "# i.e. a single-column array, where each item in the column has the pixel RGB value\n",
    "#image_expanded = np.expand_dims(color_image, axis=0)\n",
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947356038591627\n"
     ]
    }
   ],
   "source": [
    "depth_hand = depth[ymin_depth:ymax_depth,xmin_depth:xmax_depth].astype(float)\n",
    "depth_hand_scaled = depth_hand * depth_scale\n",
    "dist,_,_,_ = cv2.mean(depth_hand_scaled)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe.stop()\n",
    "pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update color and depth frames:\n",
    "#aligned_depth_frame = frameset.get_depth_frame()\n",
    "\n",
    "def showImages(color_frame, aligned_depth_frame):\n",
    "    colorizer = rs.colorizer()\n",
    "\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "    color = np.asanyarray(color_frame.get_data())\n",
    "    # Show the two frames together:\n",
    "    images = np.hstack((color, colorized_depth))\n",
    "    plt.imshow(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showBoxonDepth( depth_frame, xmin_depth,xmax_depth,ymin_depth,ymax_depth):\n",
    "    colorizer = rs.colorizer()\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "    plt.imshow(colorized_depth)\n",
    "    cv2.rectangle(colorized_depth, (xmin_depth, ymin_depth), \n",
    "                 (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "    plt.imshow(colorized_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands: \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # BGR 2 RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Flip on horizontal\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        # Set flag\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Detections\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        # Set flag to true\n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        # RGB 2 BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Detections\n",
    "        #print(results)\n",
    "        image_height, image_width, _ = image.shape\n",
    "        \n",
    "        \n",
    "        # Rendering results\n",
    "        if results.multi_hand_landmarks:\n",
    "           # print(results.multi_hand_landmarks)\n",
    "           \n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                xIndexFinger =hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width\n",
    "                yIndexFinger = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y* image_height\n",
    "                xMiddleFinger =hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].x * image_width\n",
    "                yMiddleFinger = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y* image_height\n",
    "                cv2.putText(image, 'o', (int(xIndexFinger)-10,int(yIndexFinger)-10), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(image, 'o', (int(xMiddleFinger)-10,int(yMiddleFinger)-10), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 0), 4, cv2.LINE_AA)\n",
    "               \n",
    "               \n",
    "            \n",
    "        \n",
    "        cv2.imshow('Hand Tracking', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
